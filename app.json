[{"name": "app.py", "content": "from shiny import App, ui, render, reactive\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\nfrom shinywidgets import render_plotly, output_widget, render_widget \nfrom data_process import build_active_days_df, get_window_sums, add_rolling_avg, filter_data\nfrom fetch_data import fetch_data\n\n# UI\napp_ui = ui.page_fluid(\n    ui.row(\n        ui.column(4,ui.input_password(\"token\",\"Token\")),\n        ui.column(2,ui.input_action_button(\"fetch_button\", \"Fetch Data\")),\n        id=\"token_input_row\"),\n    ui.row(\n        ui.column(6,ui.input_date(\"start_date\", \"Start Date (Min 2023-06-01)\", value=\"2023-06-01\",\n                   min=\"2023-06-01\", max=\"2024-04-24\")),\n        ui.column(6,ui.input_date(\"end_date\", \"End Date (Max 2024-04-24)\", value=\"2024-04-24\",\n                  min=\"2023-06-01\", max=\"2024-04-24\")),\n        ui.column(6,ui.input_slider(\"window\", \"Window (days)\", min=3, max=60, value =30)),\n        ui.column(6,ui.input_select(\"os\", \"OS\", [\"all\",\"undefined\",\"android\"],selected=\"all\"))),\n\n    output_widget(\"plot\")\n)\n\n# Server logic\ndef server(input, output, session):\n    # Initial fetching and preprocessing on load\n    is_loading = reactive.Value(False)\n    initial_data = reactive.Value(None)\n    reactive_data = reactive.Value(None)\n    \n\n    @reactive.Effect\n    @reactive.event(input.start_date,input.end_date, initial_data)\n    def date_trigger():\n        start_date = input.start_date()\n        end_date = input.end_date()\n\n        if (initial_data.get() is not None):\n            reactive_data.set(filter_data(initial_data.get(),start_date,end_date))\n    \n    @reactive.Effect\n    @reactive.event(is_loading)\n    def loading_trigger():\n        if not is_loading.get():\n            ui.modal_remove()\n\n    @reactive.Effect\n    @reactive.event(input.fetch_button)\n    def fetch_trigger():\n        if not is_loading.get():\n            m = ui.modal(\n                \"This may take some time\",\n                title=\"Downloading and processing data...\",\n                easy_close=False,\n                footer=None,\n            )\n            ui.modal_show(m)\n            is_loading.set(True)\n        \n            response = fetch_data(input.token())\n\n            initial_data.set(build_active_days_df(response, \"2023-06-01\",\"2024-04-24\")) \n            print(\"Data Loaded\")\n\n            is_loading.set(False)\n            ui.remove_ui(\"#token_input_row\")\n            \n\n    @output\n    @render_plotly\n    @reactive.event(reactive_data, input.window, input.os)\n    def plot():\n        if (reactive_data.get() is None):\n            return\n\n        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n        os = input.os()\n        os_column = \"active_unique_user_n\" if os == \"all\" else \\\n                    \"os_android_n\" if os == \"android\" else\\\n                    \"os_undefined_n\"\n\n        thirty_day_users = get_window_sums(reactive_data.get(), input.window())\n        active_dates_rma = add_rolling_avg(reactive_data.get(), input.window(), os_column)\n\n        # Daily counts\n        fig.add_trace(\n            go.Scatter(x=active_dates_rma['date'], \n                       y=active_dates_rma[os_column],\n                       name='Daily Counts', mode='lines', line=dict(color='red', width=2), \n                       opacity=0.5),\n            secondary_y=False,\n        )\n        \n        # RMA\n        fig.add_trace(\n            go.Scatter(x=active_dates_rma['date'], \n                       y=active_dates_rma['rolling_window_n'],\n                       name='Window RMA', mode='lines', \n                       line=dict(color='red', width=4)),\n            secondary_y=False,\n        )\n\n        # Bar window\n        fig.add_trace(\n            go.Bar(x=thirty_day_users['date_start'], \n                   y=thirty_day_users[os_column],\n                   name='End of Window Sum', marker_color='blue', opacity=0.5,\n                   width=24*60*60*1000*input.window()), # in millisecondsseconds\n            secondary_y=True,\n        )\n\n        # y-axes labels\n        fig.update_yaxes(title_text=\"Daily Unique Active Users\", secondary_y=False, tickfont=dict(color='red'))\n        fig.update_yaxes(title_text=\"Window Sum of Unique Active Users\", secondary_y=True, tickfont=dict(color='blue'))\n\n        # Update layout and add legend\n        fig.update_layout(\n            title='Analysis of User Activity',\n        )\n\n        return fig\n    \n# Shiny app\napp = App(app_ui, server)\n\nif __name__ == \"__main__\":\n    app.run()\n", "type": "text"}, {"name": "data_process.py", "content": "import pandas as pd\nimport numpy as np\nimport datetime\n\n\n# Parse login dates, which appear to be string representation of an array of datetime objects\ndef parse_login_dates(str_encoded_dates):\n    str_dates_arr = eval(str_encoded_dates) # Unsafe, for purposes of assignment\n\n    return str_dates_arr\n\n# Remove rows if missing values for certain columns\ndef clean_user_facts(user_facts_df):\n    cleaned_df = user_facts_df.dropna(subset=[\"experiment_cohort_id\",\"experiment_id\",\"loginCounter\",\"logDaysCounter\",\"loginDates\",\"lastLogin\",\"patient_id\",\"course_id\"])\n    # Add further cleaning/ validation logic as required\n\n    return cleaned_df\n\ndef group_by_user(user_facts_df):\n    grouped_by_user = user_facts_df.groupby(\"patient_id\")[['loginDates','$android_os']].agg(lambda x: list(x)).reset_index()\n\n    return grouped_by_user\n\n# Build a table with dates as rows and metrics as columns\ndef build_active_days_df(user_facts_df, start_date_ymd=\"2020-01-01\", end_date_ymd=\"2024-01-01\"):\n\n    start_date = datetime.datetime.strptime(start_date_ymd, '%Y-%m-%d')\n    end_date = datetime.datetime.strptime(end_date_ymd, '%Y-%m-%d')\n\n    cleaned_df = clean_user_facts(user_facts_df)\n\n    # Group so that each row is a unique user, aggregate login dates into one array\n    grouped_by_user = group_by_user(cleaned_df)\n\n    # Setup active dates dictionary which will become the dataframe\n    counter_date = start_date\n    dates_dict = {}\n\n    os_undefined_dict = {} # Precalculate counts for os filtering (better dashboard interactive performance)\n    os_android_dict = {}\n\n    # Generate all dates from start to end date\n    while counter_date <= end_date:\n        date_str_value = counter_date.strftime('%Y-%m-%d')\n\n        dates_dict[date_str_value] = 0 # Alternatively could hold array of user ids active on day\n        os_undefined_dict[date_str_value] = 0\n        os_android_dict[date_str_value] = 0\n\n        counter_date += datetime.timedelta(days=1)\n\n    # Takes an array of datetime objects for an unique user and processes dates dictionary\n    def process_active_dates(row):\n        datetime_arr = row[\"loginDates\"]\n        user_os = max(set(row[\"$android_os\"]), key=row[\"$android_os\"].count) # Mode from list e.g. [\"Android\",\"Android\",\"undefined\"] > \"Android\"\n\n        date_str_arr = []\n\n        for datetime_str_arr in datetime_arr:\n            try:\n                # Cannot guarantee valid python code in string or date object creation\n                login_dates_arr = parse_login_dates(datetime_str_arr)\n                login_dates_str_arr = [date_item.strftime('%Y-%m-%d') for date_item in login_dates_arr]\n                date_str_arr.extend(login_dates_str_arr)\n            except:\n                continue\n\n        date_str_set = list(set(date_str_arr)) # Remove duplicate dates\n\n        for date_str in date_str_set:\n            if date_str in dates_dict:\n                dates_dict[date_str]+=1 # Only if within dates in argument\n                if user_os == \"Android\":\n                    os_android_dict[date_str]+=1\n                else:\n                    os_undefined_dict[date_str]+=1\n\n    \n    # Process active dates by user\n    grouped_by_user.apply(process_active_dates, axis=1)\n    \n    active_dates_df = pd.DataFrame({\"date\":dates_dict.keys(),\n                                    \"active_unique_user_n\":dates_dict.values(),\n                                    \"os_android_n\":os_android_dict.values(),\n                                    \"os_undefined_n\": os_undefined_dict.values()})\n\n    return active_dates_df\n\n# Splits the days into 30d sections starting at start date, calculates the total active users for each section\ndef get_window_sums(active_dates_df, window=30):\n    copy_df = pd.DataFrame(active_dates_df)\n\n    basket_indexes = [(index+1)//window for index in active_dates_df.index]\n\n    copy_df[\"basket_window_index\"] = basket_indexes\n\n    grouped_baskets = copy_df.groupby(\"basket_window_index\").sum(numeric_only=True).reset_index()\n\n    grouped_baskets[\"date_start\"]=grouped_baskets[\"date\"].map(lambda x: x[:10]) #first 10 characters is yyyy-mm-dd of start date\n\n    return grouped_baskets.drop(columns=[\"date\"])\n\ndef add_rolling_avg(active_dates_df, window=30, os_column=\"active_unique_user_n\"):\n    copy_df = pd.DataFrame(active_dates_df)\n\n    rolling_window = active_dates_df[os_column].rolling(window).mean()\n\n    copy_df[\"rolling_window_n\"]=rolling_window\n\n    return copy_df\n\ndef filter_data(active_dates_df, start_date, end_date):\n    dates_col=pd.to_datetime(active_dates_df[\"date\"])\n\n    start_date_np = pd.to_datetime(start_date)\n    \n    end_date_np = pd.to_datetime(end_date)\n    \n    return active_dates_df[(dates_col>=start_date_np) & (dates_col<=end_date_np)]\n\n", "type": "text"}, {"name": "fetch_data.py", "content": "import pandas as pd\n#from pathlib import Path\nimport requests\nfrom io import StringIO\n\ndef fetch_data(token=\"\"):\n    url = f\"https://api.github.com/repos/georgezeng0/limbic_da_assignment/contents/raw_data/user_facts.csv\"\n    headers = {\n        \"Authorization\": f\"token {token}\",\n        \"Accept\": \"application/vnd.github.v3.raw\"\n    }\n\n    try:\n        #user_facts = pd.read_csv(Path(__file__).parent / \"user_facts.csv\") # Local file\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            return pd.read_csv(StringIO(response.text))\n        else:\n            raise Exception(f\"Failed to download file: {response.status_code} - {response.text}\")\n    except Exception as e:\n        print(f\"Error Fetching Data: {e}\")\n", "type": "text"}]